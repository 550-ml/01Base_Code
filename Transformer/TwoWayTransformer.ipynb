{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import contextlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Type\n",
    "import torch.nn.functional as F\n",
    "from misc import get_sdpa_settings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n",
    "ALLOW_ALL_KERNELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        activation: nn.Module = nn.ReLU,\n",
    "        sigmoid_dropout: bool = False\n",
    "    ) -> None:\n",
    "        self.num_layers = num_layers\n",
    "        # list\n",
    "        h = [hidden_dim] * (num_layers-1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n,k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_dropout\n",
    "        self.act = activation()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x\n",
    "    ):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = self.act(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import intern\n",
    "\n",
    "\n",
    "def sdp_kernel_context(dropout_p):\n",
    "    \"\"\"\n",
    "    Get the context for the attention scaled dot-product kernel. We use Flash Attention\n",
    "    by default, but fall back to all available kernels if Flash Attention fails.\n",
    "    \"\"\"\n",
    "    if ALLOW_ALL_KERNELS:\n",
    "        return contextlib.nullcontext()\n",
    "\n",
    "    return torch.backends.cuda.sdp_kernel(\n",
    "        enable_flash=USE_FLASH_ATTN,\n",
    "        # if Flash attention kernel is off, then math kernel needs to be enabled\n",
    "        enable_math=(OLD_GPU and dropout_p > 0.0) or MATH_KERNEL_ON,\n",
    "        enable_mem_efficient=OLD_GPU,\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"attention layer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        kv_in_dim: int = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.kv_in_dim = kv_in_dim if kv_in_dim is not None else embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.internal_dim % self.num_heads == 0\n",
    "        ), \"number of heads must divide internal dimension\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        batch_size, seq_len, internal_dim = x.shape\n",
    "        x = x.view(batch_size, seq_len, num_heads, internal_dim // num_heads).permute(0, 2, 1, 3)\n",
    "        return x # B * N_heads * Sequence_len * dim_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, dim_per_head = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).reshape(b, n_tokens, n_heads * dim_per_head)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input Projection\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # multi-head\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        dropout_p = self.dropout_p if self.training else 0.0\n",
    "        try:\n",
    "            with sdp_kernel_context(dropout_p):\n",
    "                out = F.scaled_dot_product_attention(q, k, v, dropout_p)\n",
    "        except Exception as e:\n",
    "            warnings.warn(\n",
    "                f\"Flash Attention kernel failed due to: {e}\\nFalling back to all available \"\n",
    "                f\"kernels for scaled_dot_product_attention (which may have a slower speed).\",\n",
    "                category=UserWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            global ALLOW_ALL_KERNELS\n",
    "            ALLOW_ALL_KERNELS = True\n",
    "            out = F.scaled_dot_product_attention(q, k, v, dropout_p)\n",
    "        \n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import pen\n",
    "\n",
    "\n",
    "class TwoWayAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int = 2048,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_dowansample_rate: int = 2,\n",
    "        skip_first_layer_pe: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Tansformer block有四个层：\n",
    "        1. 稀疏查询自注意力\n",
    "        2. 稀疏到稠密查询交叉注意力\n",
    "        3. mlp稀疏查询\n",
    "        4. 密集查询到稀疏查询的交叉注意力\n",
    "        \n",
    "\n",
    "        Args:\n",
    "            embedding_dim (int): _description_\n",
    "            num_heads (int): _description_\n",
    "            mlp_dim (int, optional): _description_. Defaults to 2048.\n",
    "            activation (Type[nn.Module], optional): _description_. Defaults to nn.ReLU.\n",
    "            attention_dowansample_rate (int, optional): _description_. Defaults to 2.\n",
    "            skip_first_layer_pe (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embedding_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.cross_attn_token_to_image = Attention(\n",
    "            embedding_dim = embedding_dim,\n",
    "            num_heads = num_heads,\n",
    "            downsample_rate=attention_dowansample_rate\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_dim=embedding_dim,\n",
    "            hidden_dim=mlp_dim,\n",
    "            output_dim=embedding_dim,\n",
    "            num_layers=2,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.cross_attn_image_to_token = Attention(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            downsample_rate=attention_dowansample_rate\n",
    "        )\n",
    "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.skip_first_layer_pe = skip_first_layer_pe\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        # point_embedding, image_embedding, poing\n",
    "        # self attention block\n",
    "        if self.skip_first_layer_pe:\n",
    "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
    "        else:\n",
    "            q = queries + query_pe\n",
    "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
    "            queries = attn_out + queries\n",
    "        queries = self.norm1(queries)\n",
    "        \n",
    "        # cross_attn\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm2(queries)\n",
    "\n",
    "        # mlp\n",
    "        mlp_out = self.mlp(queries)\n",
    "        queries = queries + mlp_out\n",
    "        queries = self.norm3(queries)\n",
    "        \n",
    "        # cross\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
    "        keys = keys + attn_out\n",
    "        keys = self.norm4(keys)\n",
    "        \n",
    "        return queries, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoWayTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim:int,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"一个双向的transformer，用于处理两个不同的输入，然后输出一个结果\n",
    "\n",
    "        Args:\n",
    "            depth (int): layer的数量\n",
    "            embedding_dim (int): 编码的维度\n",
    "            num_heads (int): 多头注意力\n",
    "            mlp_dim (int): mlp的维度\n",
    "            activation (Type[nn.Module], optional): _description_. Defaults to nn.ReLU.\n",
    "            attention_downsample (int, optional): _description_. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                TwoWayAttentionBlock(\n",
    "                    \n",
    "            )\n",
    "    def forward(\n",
    "        self,\n",
    "        \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
