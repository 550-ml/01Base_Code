{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, Type\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_sdpa_settings\n\u001b[1;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON \u001b[38;5;241m=\u001b[39m get_sdpa_settings()\n",
      "File \u001b[0;32m~/Myproject/01Python_project/01Base_Code/Transformer/misc.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sdpa_settings\u001b[39m():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import contextlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Type\n",
    "import torch.nn.functional as F\n",
    "from misc import get_sdpa_settings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n",
    "ALLOW_ALL_KERNELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import intern\n",
    "\n",
    "\n",
    "def sdp_kernel_context(dropout_p):\n",
    "    \"\"\"\n",
    "    Get the context for the attention scaled dot-product kernel. We use Flash Attention\n",
    "    by default, but fall back to all available kernels if Flash Attention fails.\n",
    "    \"\"\"\n",
    "    if ALLOW_ALL_KERNELS:\n",
    "        return contextlib.nullcontext()\n",
    "\n",
    "    return torch.backends.cuda.sdp_kernel(\n",
    "        enable_flash=USE_FLASH_ATTN,\n",
    "        # if Flash attention kernel is off, then math kernel needs to be enabled\n",
    "        enable_math=(OLD_GPU and dropout_p > 0.0) or MATH_KERNEL_ON,\n",
    "        enable_mem_efficient=OLD_GPU,\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"attention layer\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        kv_in_dim: int = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.kv_in_dim = kv_in_dim if kv_in_dim is not None else embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.internal_dim % self.num_heads == 0\n",
    "        ), \"number of heads must divide internal dimension\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        batch_size, seq_len, internal_dim = x.shape\n",
    "        x = x.view(batch_size, seq_len, num_heads, internal_dim // num_heads).permute(0, 2, 1, 3)\n",
    "        return x # B * N_heads * Sequence_len * dim_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, dim_per_head = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).reshape(b, n_tokens, n_heads * dim_per_head)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input Projection\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # multi-head\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        dropout_p = self.dropout_p if self.training else 0.0\n",
    "        try:\n",
    "            with sdp_kernel_context(dropout_p):\n",
    "                out = F.scaled_dot_product_attention(q, k, v, dropout_p)\n",
    "        except Exception as e:\n",
    "            warnings.warn(\n",
    "                f\"Flash Attention kernel failed due to: {e}\\nFalling back to all available \"\n",
    "                f\"kernels for scaled_dot_product_attention (which may have a slower speed).\",\n",
    "                category=UserWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            global ALLOW_ALL_KERNELS\n",
    "            ALLOW_ALL_KERNELS = True\n",
    "            out = F.scaled_dot_product_attention(q, k, v, dropout_p)\n",
    "        \n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoWayAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int = 2048,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_dowansample_rate: int = 2,\n",
    "        skip_first_layer_pe: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Tansformer block有四个层：\n",
    "        1. 稀疏查询自注意力\n",
    "        2. 稀疏到稠密查询交叉注意力\n",
    "        3. mlp稀疏查询\n",
    "        4. 密集查询到稀疏查询的交叉注意力\n",
    "        \n",
    "\n",
    "        Args:\n",
    "            embedding_dim (int): _description_\n",
    "            num_heads (int): _description_\n",
    "            mlp_dim (int, optional): _description_. Defaults to 2048.\n",
    "            activation (Type[nn.Module], optional): _description_. Defaults to nn.ReLU.\n",
    "            attention_dowansample_rate (int, optional): _description_. Defaults to 2.\n",
    "            skip_first_layer_pe (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoWayTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim:int,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"一个双向的transformer，用于处理两个不同的输入，然后输出一个结果\n",
    "\n",
    "        Args:\n",
    "            depth (int): layer的数量\n",
    "            embedding_dim (int): 编码的维度\n",
    "            num_heads (int): 多头注意力\n",
    "            mlp_dim (int): mlp的维度\n",
    "            activation (Type[nn.Module], optional): _description_. Defaults to nn.ReLU.\n",
    "            attention_downsample (int, optional): _description_. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                TwoWayAttnetionBlock(\n",
    "                    \n",
    "            )\n",
    "    def forward(\n",
    "        self,\n",
    "        \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
