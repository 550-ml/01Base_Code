{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18896\\2405063774.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mMemoryAttentionLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mactivation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcross_attention\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MemoryAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation: str,\n",
    "        cross_attention: nn.Module,\n",
    "        d_model: int,  # embedding 的维度\n",
    "        dim_feedforward: int,\n",
    "        dropout: float,\n",
    "        pos_enc_at_attn: bool, # 添加位置编码，自注意力\n",
    "        pos_enc_at_cross_attn_keys: bool,\n",
    "        pos_enc_at_cross_attn_queries: bool,\n",
    "        self_attention: nn.Module,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout_value = dropout\n",
    "        self.self_attn = self_attention\n",
    "        self.cross_attn_image = cross_attention\n",
    "\n",
    "        # feadforward\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation_str = activation\n",
    "        self.activation = get_activation_fn(activation)\n",
    "        \n",
    "        # pos enc\n",
    "        self.pos_enc_at_attn = pos_enc_at_attn\n",
    "        self.pos_enc_at_cross_attn_queries = pos_enc_at_cross_attn_queries\n",
    "        self.pos_enc_at_cross_attn_keys = pos_enc_at_cross_attn_keys\n",
    "    \n",
    "    def _forward_sa(self, tgt, query_pos):\n",
    "        # 自注意力\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = tgt2 + query_pos if self.pos_enc_at_attn else tgt2\n",
    "        tgt2 = self.self_attn(q=q, k=k, v=tgt2)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        return tgt\n",
    "    \n",
    "    def _forward_ca(self, tgt, memory, query_pos, pos, num_k_exclude_rope=0):\n",
    "        kwds = {}\n",
    "        if num_k_exclude_rope > 0:\n",
    "            assert isinstance(self.cross_attn_image, RoPEAttention)\n",
    "            kwds = {\"num_k_exclude_rope\": num_k_exclude_rope}\n",
    "        \n",
    "        # 交叉注意力\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.cross_attn_image(\n",
    "            q=tgt2 + query_pos if self.pos_enc_at_cross_attn_queries else tgt2,\n",
    "            k=memory + pos if self.pos_enc_at_cross_attn_keys else memory,\n",
    "            v=memory,\n",
    "            **kwds,\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        pos: Optional[Tensor] = None,\n",
    "        query_pos: Optional[Tensor] = None,\n",
    "        num_k_exclude_rope: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # Self-Attn, Cross-Attn\n",
    "        tgt = self._forward_sa(tgt, query_pos)\n",
    "        tgt = self._forward_ca(tgt, memory, query_pos, pos, num_k_exclude_rope)\n",
    "        # MLP\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        pos_enc_at_input: bool,\n",
    "        layer: nn.Module,\n",
    "        num_layers: int,\n",
    "        batch_first: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layers = get_clones(layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.pos_enc_at_input = pos_enc_at_input\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        curr: torch.Tensor, # self-attention\n",
    "        memory: torch.Tensor, # cross-attentio inputs\n",
    "        curr_pos: Optional[Tensor] = None,  # pos_enc for self-attention inputs\n",
    "        memory_pos: Optional[Tensor] = None,  # pos_enc for cross-attention inputs\n",
    "        num_obj_ptr_tokens: int = 0,  # number of object pointer *tokens*\n",
    "    ):\n",
    "        if isinstance(curr, list):\n",
    "            assert isinstance(curr_pos, list)\n",
    "            assert len(curr) == len(curr_pos) == 1\n",
    "            curr, curr_pos = (\n",
    "                curr[0],\n",
    "                curr_pos[0],\n",
    "            )\n",
    "\n",
    "        assert (\n",
    "            curr.shape[1] == memory.shape[1]\n",
    "        ), \"Batch size must be the same for curr and memory\"\n",
    "        \n",
    "        output = curr\n",
    "        if self.pos_enc_at_input and curr_pos is not None:\n",
    "            output = output + 0.1 * curr_pos\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Convert to batch first\n",
    "            output = output.transpose(0, 1)\n",
    "            curr_pos = curr_pos.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            memory_pos = memory_pos.transpose(0, 1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            kwds = {}\n",
    "            if isinstance(layer.cross_attn_image, RoPEAttention):\n",
    "                kwds = {\"num_k_exclude_rope\": num_obj_ptr_tokens}\n",
    "\n",
    "            output = layer(\n",
    "                tgt=output,\n",
    "                memory=memory,\n",
    "                pos=memory_pos,\n",
    "                query_pos=curr_pos,\n",
    "                **kwds,\n",
    "            )\n",
    "        normed_output = self.norm(output)\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Convert back to seq first\n",
    "            normed_output = normed_output.transpose(0, 1)\n",
    "            curr_pos = curr_pos.transpose(0, 1)\n",
    "\n",
    "        return normed_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
